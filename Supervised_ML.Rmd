---
title: "Supervised_ML"
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    collapsed: false
    number_sections: true
    toc_depth: 3
    #code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message=FALSE,warning=FALSE, cache=TRUE)
```

# Supervised ML - Regression


## Challenge

```{r}
# Standard
library(tidyverse)

# Modeling
library(parsnip)

# Preprocessing & Sampling
library(recipes)
library(rsample)

# Modeling Error Metrics
library(yardstick)

# Plotting Decision Trees
library(rpart.plot)
library(tidymodels)
library(broom.mixed)

#I. Build a model ----------------------------------------------------------------

bike_features_tbl <- readRDS("00_data/Business Decisions with Machine Learning/bike_features_tbl.rds")
bike_features_data <- bike_features_tbl %>% 
  select(model:gender, `Rear Derailleur`, `Shift Lever`) 

set.seed(seed = 1123)

# LINEAR REGRESSION -
model_linear_reg <- linear_reg("regression") %>%
  set_engine("lm")
split_obj <- rsample::initial_split(bike_features_data, prop   = 0.80, 
                                    strata = "category_2")

train_tbl <- training(split_obj)
test_tbl  <- testing(split_obj)

train_tbl <- train_tbl %>% set_names(str_replace_all(names(train_tbl), " |-", "_"))
test_tbl  <- test_tbl  %>% set_names(str_replace_all(names(test_tbl),  " |-", "_"))

#II. Create features with the recipes package--------
bike_recipe <- recipe_obj <- 
  recipe(price  ~ ., data = train_tbl %>% select(-c(model:weight), -category_1, -c(category_3:gender))) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% 
  prep()

summary(bike_recipe)

train_transformed_tbl <- bake(bike_recipe, new_data = NULL)
test_transformed_tbl  <- bake(bike_recipe, new_data = test_tbl)

train_transformed_tbl
test_transformed_tbl

#III. Bundle the model and recipe with the workflow package
bike_workflow <-  workflow() %>% add_model(model_linear_reg) %>% add_recipe(bike_recipe)

bike_fit <- bike_workflow %>% fit(data = train_tbl)

#IV. Evaluate your model with the yardstick package
predict(bike_fit, test_tbl) %>%
  
  bind_cols(test_tbl %>% select(price)) %>%
  yardstick::metrics(truth = price, estimate = .pred)

```

## Business case

```{r}

# Standard
library(tidyverse)

# Modeling
library(parsnip)

# Preprocessing & Sampling
library(recipes)
library(rsample)

# Modeling Error Metrics
library(yardstick)

# Plotting Decision Trees
library(rpart.plot)

# Modeling ----------------------------------------------------------------
bike_orderlines_tbl <- readRDS("00_data/Business Decisions with Machine Learning/bike_orderlines.rds")
#glimpse(bike_orderlines_tbl)

model_sales_tbl <- bike_orderlines_tbl %>%
  select(total_price, model, category_2, frame_material) %>%
  
  group_by(model, category_2, frame_material) %>%
  summarise(total_sales = sum(total_price)) %>%
  ungroup() %>%
  
  arrange(desc(total_sales))

model_sales_tbl %>%
  mutate(category_2 = as_factor(category_2) %>% 
           fct_reorder(total_sales, .fun = max) %>% 
           fct_rev()) %>%
  
  ggplot(aes(frame_material, total_sales)) +
  geom_violin() +
  geom_jitter(width = 0.1, alpha = 0.5, color = "#2c3e50") +
  #coord_flip() +
  facet_wrap(~ category_2) +
  scale_y_continuous(labels = scales::dollar_format(scale = 1e-6, suffix = "M", accuracy = 0.1)) +
  tidyquant::theme_tq() +
  labs(
    title = "Total Sales for Each Model",
    x = "Frame Material", y = "Revenue"
  )

bike_features_tbl <- readRDS("00_data/Business Decisions with Machine Learning/bike_features_tbl.rds")
glimpse(bike_features_tbl)

bike_features_tbl <- bike_features_tbl %>% 
  select(model:url, `Rear Derailleur`, `Shift Lever`) %>% 
  mutate(
    `shimano dura-ace`        = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano dura-ace ") %>% as.numeric(),
    `shimano ultegra`         = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano ultegra ") %>% as.numeric(),
    `shimano 105`             = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano 105 ") %>% as.numeric(),
    `shimano tiagra`          = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano tiagra ") %>% as.numeric(),
    `Shimano sora`            = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano sora") %>% as.numeric(),
    `shimano deore`           = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano deore(?! xt)") %>% as.numeric(),
    `shimano slx`             = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano slx") %>% as.numeric(),
    `shimano grx`             = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano grx") %>% as.numeric(),
    `Shimano xt`              = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano deore xt |shimano xt ") %>% as.numeric(),
    `Shimano xtr`             = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano xtr") %>% as.numeric(),
    `Shimano saint`           = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano saint") %>% as.numeric(),
    `SRAM red`                = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram red") %>% as.numeric(),
    `SRAM force`              = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram force") %>% as.numeric(),
    `SRAM rival`              = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram rival") %>% as.numeric(),
    `SRAM apex`               = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram apex") %>% as.numeric(),
    `SRAM xx1`                = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram xx1") %>% as.numeric(),
    `SRAM x01`                = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram x01|sram xo1") %>% as.numeric(),
    `SRAM gx`                 = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram gx") %>% as.numeric(),
    `SRAM nx`                 = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram nx") %>% as.numeric(),
    `SRAM sx`                 = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram sx") %>% as.numeric(),
    `SRAM sx`                 = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram sx") %>% as.numeric(),
    `Campagnolo potenza`      = `Rear Derailleur` %>% str_to_lower() %>% str_detect("campagnolo potenza") %>% as.numeric(),
    `Campagnolo super record` = `Rear Derailleur` %>% str_to_lower() %>% str_detect("campagnolo super record") %>% as.numeric(),
    `shimano nexus`           = `Shift Lever`     %>% str_to_lower() %>% str_detect("shimano nexus") %>% as.numeric(),
    `shimano alfine`          = `Shift Lever`     %>% str_to_lower() %>% str_detect("shimano alfine") %>% as.numeric()
  ) %>% 
  # Remove original columns  
  select(-c(`Rear Derailleur`, `Shift Lever`)) %>% 
  # Set all NAs to 0
  mutate_if(is.numeric, ~replace(., is.na(.), 0))    

# 2.0 TRAINING & TEST SETS ----
bike_features_tbl <- bike_features_tbl %>% 
  
  mutate(id = row_number()) %>% 
  
  select(id, everything(), -url)
bike_features_tbl %>% distinct(category_2)

# run both following commands at the same time
set.seed(seed = 1113)
split_obj <- rsample::initial_split(bike_features_tbl, prop   = 0.80, 
                                    strata = "category_2")

# Check if testing contains all category_2 values
split_obj %>% training() %>% distinct(category_2)
split_obj %>% testing() %>% distinct(category_2)

# Assign training and test data
train_tbl <- training(split_obj)
test_tbl  <- testing(split_obj)

# We have to remove spaces and dashes from the column names
train_tbl <- train_tbl %>% set_names(str_replace_all(names(train_tbl), " |-", "_"))
test_tbl  <- test_tbl  %>% set_names(str_replace_all(names(test_tbl),  " |-", "_"))

#glimpse(test_tbl)
# 3.0 LINEAR METHODS ----
# 3.1 LINEAR REGRESSION - NO ENGINEERED FEATURES ----

# 3.1.1 Model ----
#?lm # from the stats package
#?set_engine
#?fit # then click Estimate model parameters and then fit at the bottom
model_01_linear_lm_simple <- linear_reg(mode = "regression") %>%
  set_engine("lm") %>%
  fit(price ~ category_2 + frame_material, data = train_tbl)

# model_01_linear_lm_simple %>%
#   predict(new_data = (test_tbl(na.lm =TRUE))) %>%
#                                         
# bind_cols(test_tbl %>% select(price)) %>%
#   yardstick::metrics(truth = price, estimate = .pred)

 # 3.1.2 Feature Importance ----
#View(model_01_linear_lm_simple) # You will see the coefficients in the element "fit"
model_01_linear_lm_simple$fit %>% class()

model_01_linear_lm_simple$fit %>%
  broom::tidy() %>%
  arrange(p.value) %>%
  mutate(term = as_factor(term) %>% fct_rev()) %>%
  
  ggplot(aes(x = estimate, y = term)) +
  geom_point(color = "#2dc6d6", size = 3) +
  ggrepel::geom_label_repel(aes(label = scales::dollar(estimate, accuracy = 1, suffix = " €", prefix = "")),
                            size = 4, fill = "#272A36", color = "white") +
  scale_x_continuous(labels = scales::dollar_format(suffix = " €", prefix = "")) +
  labs(title = "Linear Regression: Feature Importance",
       subtitle = "Model 01: Simple lm Model") 

# 3.1.3 Function to Calculate Metrics ----


# Generalized into a function
calc_metrics <- function(model, new_data = test_tbl) {
  
  model %>%
    predict(new_data = new_data) %>%
    
    bind_cols(new_data %>% select(price)) %>%
    yardstick::metrics(truth = price, estimate = .pred)
  
}

#model_01_linear_lm_simple %>% calc_metrics(test_tbl)

# 3.2 LINEAR REGRESSION - WITH ENGINEERED FEATURES ----

# 3.2.1 Model ----
model_02_linear_lm_complex <- linear_reg("regression") %>%
  set_engine("lm") %>%
  
  # This is going to be different. Remove unnecessary columns.
  fit(price ~ ., data = train_tbl %>% select(-c(id:weight), -category_1, -c(category_3:gender)))

#model_02_linear_lm_complex %>% calc_metrics(new_data = test_tbl)

# 3.2.2 Feature importance ----
model_02_linear_lm_complex$fit %>%
  
  broom::tidy() %>%
  arrange(p.value) %>%
  mutate(term = as_factor(term) %>% fct_rev()) %>%
  
  ggplot(aes(x = estimate, y = term)) +
  geom_point(color = "#2dc6d6", size = 3) +
  ggrepel::geom_label_repel(aes(label = scales::dollar(estimate, accuracy = 1, suffix = " €", prefix = "")),
                            size = 4, fill = "#272A36", color = "white") +
  scale_x_continuous(labels = scales::dollar_format(suffix = " €", prefix = "")) +
  labs(title = "Linear Regression: Feature Importance",
       subtitle = "Model 02: Complex lm Model")

# 3.3 PENALIZED REGRESSION ----
model_03_linear_glmnet <- linear_reg(mode    = "regression", 
                                     penalty = 10, 
                                     mixture = 0.1) %>%
  set_engine("glmnet") %>%
  fit(price ~ ., data = train_tbl %>% select(-c(id:weight), -category_1, -c(category_3:gender)))
# 3.3.2 Feature Importance ----
model_03_linear_glmnet$fit %>%
  broom::tidy() %>%
  filter(lambda >= 10 & lambda < 11) %>%
  
  # No p value here
  arrange(desc(abs(estimate))) %>%
  mutate(term = as_factor(term) %>% fct_rev()) %>%
  
  ggplot(aes(x = estimate, y = term)) +
  geom_point() +
  ggrepel::geom_label_repel(aes(label = scales::dollar(estimate, accuracy = 1)),
                            size = 3) +
  scale_x_continuous(labels = scales::dollar_format()) +
  labs(title = "Linear Regression: Feature Importance",
       subtitle = "Model 03: GLMNET Model")
model_04_tree_decision_tree <- decision_tree(mode = "regression",
                                             
                                             # Set the values accordingly to get started
                                             cost_complexity = 0.001,
                                             tree_depth      = 5,
                                             min_n           = 7) %>%
  
  set_engine("rpart") %>%
  fit(price ~ ., data = train_tbl %>% select(-c(id:weight), -category_1, -c(category_3:gender)))

#model_04_tree_decision_tree %>% calc_metrics(test_tbl)

# 4.1.2 Decision Tree Plot ----
#?rpart.plot()

model_04_tree_decision_tree$fit %>%
  rpart.plot(roundint = FALSE)

# Optimze plot
model_04_tree_decision_tree$fit %>%
  rpart.plot(
    roundint = FALSE,
    type = 1,
    extra = 101, # see help page
    fallen.leaves = FALSE, # changes the angles from 90 to 45-degree
    cex = 0.8, # font size
    main = "Model 04: Decision Tree", # Adds title
    box.palette = "Blues"
  )


library(ranger)

set.seed(1234)
model_05_rand_forest_ranger <- rand_forest(
  mode = "regression", mtry = 8, trees = 5000, min_n = 10
) %>%
  
  # Run ?ranger::ranger to play around with many arguments
  # We need to set importance to impurity to be able to explain the model in the next step
  set_engine("ranger", replace = TRUE, splitrule = "extratrees", importance = "impurity") %>%
  fit(price ~ ., data = train_tbl %>% select(-c(id:weight), -category_1, -c(category_3:gender)))

#model_05_rand_forest_ranger %>% calc_metrics(test_tbl)

# 4.2.2 ranger: Feature Importance ----

model_05_rand_forest_ranger$fit %>%
  ranger::importance() %>%
  enframe() %>%
  arrange(desc(value)) %>%
  mutate(name = as_factor(name) %>% fct_rev()) %>%
  
  ggplot(aes(value, name)) +
  geom_point() +
  labs(title = "ranger: Variable Importance",
       subtitle = "Model 05: Ranger Random Forest Model")

# 5.0 TESTING THE ALGORITHMS OUT ----
g1 <- bike_features_tbl %>% 
  mutate(category_2 = as.factor(category_2) %>% 
           fct_reorder(price)) %>% 
  
  ggplot(aes(category_2, price)) +
  geom_violin() +
  geom_jitter(width = 0.1, alpha = 0.5, color = "#2dc6d6") +
  coord_flip() +
  facet_wrap(~ frame_material) +
  scale_y_continuous(labels = scales::dollar_format()) +
  labs(
    title = "Unit Price for Each Model",
    y = "", x = "Category 2"
  )


```

